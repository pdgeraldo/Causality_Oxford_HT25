---
title: "Potential outcomes to hypothesize, experiments to learn"
author: "Pablo Geraldo Bast√≠as"
logo: "https://fundit.fr/sites/default/files/styles/max_650x650/public/institutions/capture-decran-2023-07-07-162216.png?itok=1CkwlJEu"
include-in-header:
  - text: |
      <style>
      .reveal .slide-logo {
        max-height: unset;
        height: 100px;
      }
      </style>
footer: "Potential outcomes and experiments"
date: 01/20/2025
date-format: long
format: 
  revealjs:
    theme: simple
    width: 1600
    height: 900
    transition: slide
    slide-number: c/t
    chalkboard: true
    auto-stretch: false
callout-appearance: simple
---

# Outline {background-color="#17a091"}

\newcommand\indep{\perp\!\!\!\perp}
\newcommand\nindep{\not\!\perp\!\!\!\perp}

## From causal methods to causal assumptions

It is common to hear the idea that some *methods* are inherently causal: propensity score matching, IPW, instrumental variables, panel methods, and even machine learning

But there is nothing properly causal about a method (i.e, an estimator, an algorithm, a formula)!

What makes our conclusions causal is the assumptions we make, whose plausibility is justified (or not) on a given research design


## From causal methods to causal assumptions

In this first lecture, we are going to study the formal language that allows us to clearly and transparently state both our quantities of interest and our assumptions

Then we will **see** this framework in action, to understand how does it work in the context of randomised studies

We should be able to understand why RCTs are often described as the "gold standard" for causal inference

## The First Commandment of Causal Inference {background-color="#692044"}

:::{.fragment}

### (by Chad Hazlett)

::: callout-warning
Thou shalt not blindly apply identification strategies as if the *procedure* makes your results causal
:::

:::
. . .

Identification strategies are about assumptions, not tools:

-   learning tools is important, useful, and fun

-   but the tool you use doesn't make your result causal, your assumptions do

-   most of your effort should go into understanding and communicating whether your assumptions are credible

. . .

::: callout-note
## Corolary: Never say "my estimate is causal because I used (.)"

Where (.) = matching, IPW, IV, RDD, DML, whatever
:::


# Potential Outcomes {background-color="#17a091"}

## Neyman, Neyman-Rubin, or Rubin Causal Model (?)

<br>

POs were introduced by Neyman (1923) in the context of experimental design. They remained only used in that context for decades!

-   Imported and extended by Donald Rubin for observational studies (c. 1974)

-   They are really great to clarify *what do we want to know* ([[**estimand**]()]{.fragment .fade-in})

-   This includes identifying *reasons for discrepancies* between what we observe and our estimand ([[**bias**]()]{.fragment .fade-in})

-   They are great to formalize *what needs to be true* for our estimand to be identified with a given *estimator* ([[**assumptions**]()]{.fragment .fade-in})

. . .

-   They are not-so-great to assess if our assumptions are plausible or defensible (more on this later!)

------------------------------------------------------------------------

## Notation

To think *causally*, we need to go [beyond]{style="color:red;"} what we can observe. We need to hypothesize about possible worlds (*what if*s)!

. . .

To do this, let's *postulate* the existence of [potential outcomes]{style="color:red;"}

<br>

Let's start with some definitions:

. . .

$Y$ is the outcome variable *as we observe it*

$D$ is the variable whose effect we want to study (treatment, exposure)

$Y_d$ is the potential outcome when we [**set**]{.fragment .highlight-red} $D=d$. For example, when $D \in \{0,1\}$:

-   $Y_1$ is the potential outcome under "treatment"

-   $Y_0$ is the potential outcome under "control"

------------------------------------------------------------------------

## Notation

You will find a lot of equivalent notations for potential outcomes. It could be confusing, but it is good to get practice working with different variants

$$Y(d) = Y_d = Y^d$$

. . .

::: callout-tip
## Read it like this:

The value that the variable $Y$ would take if we were to set/manipulate the variable $D$ to the value $d$.
:::

::: {.fragment .r-stack}
What is the [*fundamental problem*]{style="color:red;"} of causal inference? (Holland, 1986)
:::

::: {.fragment .r-stack}
*Can you ever observe any of those potential outcomes?*
:::

::: {.fragment .r-stack}
*"Causal inference is a missing data problem"*
:::

## Notation

**Consistency** (also known as SUTVA): $$D = d \rightarrow Y = Y_d$$

For the binary treatment case, we have: $$Y = DY_1 + (1-D)Y_0$$ (a.k.a., "switching equation")

. . .

::: callout-note
What are the assumptions built in this notation? What type of dependence are we ruling out?
:::

------------------------------------------------------------------------

## Causal estimands

The first thing that potential outcomes allow us to do is to formalize the causal effects we may want to estimate.

Some frequently invoked estimands are the following: 

<br>

[[Individual treatment effect]{style="color:blue;"}
$$ITE = \tau_i = Y_{1i} - Y_{0i}$$]{.fragment .fade-in}

. . .

* How can we read this expression?
* Can we ever estimate it from data?

## Causal estimands

The first thing that potential outcomes allow us to do is to formalize the causal effects we may want to estimate.

Some frequently invoked estimands are the following: 

<br>


[Average treatment effect]{style="color:blue;"}
$$
ATE = E[\tau_i] = E[Y_{1i} - Y_{0i}]$$ $$= E[Y_{1i}] - E[Y_{0i}]
$$

. . .

* How can we read this expression?
* Can we ever estimate it from data?

## Causal estimands

The first thing that potential outcomes allow us to do is to formalize the causal effects we may want to estimate.

Some frequently invoked estimands are the following:

<br>

[Average treatment effect on the treated]{style="color:blue;"}
$$
ATT = E[\tau_i|D_i=1] = E[Y_{1i} - Y_{0i}|D_i=1]$$ $$= E[Y_{1i}|D_i=1] - E[Y_{0i}|D_i=1]
$$

. . .

* How can we read this expression?
* Can we ever estimate it from data?

## Causal estimands

The first thing that potential outcomes allow us to do is to formalize the causal effects we may want to estimate.

Some frequently invoked estimands are the following:

<br>


[Average treatment effect on the controls]{style="color:blue;"}
$$
ATC = E[\tau_i|D_i=0] = E[Y_{1i} - Y_{0i}|D_i=0]$$ $$= E[Y_{1i}|D_i=0] - E[Y_{0i}|D_i=0]
$$

. . .

* How can we read this expression?
* Can we ever estimate it from data?

## Causal estimands

The first thing that potential outcomes allow us to do is to formalize the causal effects we may want to estimate.

Some frequently invoked estimands are the following:

<br>

[Conditional average treatment effect]{style="color:blue;"}
$$
CATE = E[\tau_i|X_i=x] = E[Y_{1i} - Y_{0i}|X_i=x]$$ $$= E[Y_{1i}|X_i=x] - E[Y_{0i}|X_i=x]
$$

. . .

* How can we read this expression?
* Can we ever estimate it from data?



## Checking intuitions

Let's assume $D$ (the exposure) is defined as "doing the required readings before lecture", and $Y$ is "understanding what is going on". Then,

. . .

$$E[Y_{1i}] = E[Y_i | D_i=1]?$$

. . .

* Let's try to parse this out:

  + What is the individual quantity we are looking at?
  
  + What is the population we are targeting?
  
  + How would the observed $Y_{1i}$ (for those with $D_i=1$) compare to the $Y_{1i}$ you don't see?
  
  + [**When**]{style="color:red;"} would these two quantities be equivalent?
  

## Checking intuitions

Let's assume $D$ (the exposure) is defined as "doing the required readings before lecture", and $Y$ is "understanding what is going on". Then,



$$E[Y_{1i}] = E[Y_i | D_i=1]?$$



* If we throw $D_i$ at random in this room:

  + How would the observed $Y_{1i}$ (for those with $D_i=1$) compare to the $Y_{1i}$ you don't see?
  
  + What about the $Y_{0i}$?
  
  + Can we estimate $E[Y_1]$ and $E[Y_0]$?
  
## Checking intuitions

Let's assume $D$ (the exposure) is defined as "doing the required readings before lecture", and $Y$ is "understanding what is going on". Then,



$$E[Y_{1i}] = E[Y_i | D_i=1]?$$



* Suppose $Y_{1i} = Y_{0i}$ for everyone, and I can see them all:

  + What happens if I give $D_i=1$ more often to those with higher $Y_1$?
  
  + How would the $Y_1$ you see vs the $Y_1$ you don't see relate?
  
  + What would $E[Y_i|D_i=1] - E[Y_i | D_i=0]$ tell me?



## Short activity (3 mins) {background-color="#692044"}


Take a moment to think about your own research:

-   What **causal** question is relevant for you to study?
-   Can you formulate it using potential outcomes?
  + What is the contrast of interest?
  + Averaging over which sub-population?
-   What are the assumptions your are making in this formalization?

------------------------------------------------------------------------

## The Science Table

```{r, echo=FALSE}
data <- 
  data.frame(D=c(rep("A",5),rep("B",5)),
         Ya=c(1,0,0,1,1,1,1,0,0,1),
         Yb=c(1,0,1,0,1,0,0,0,1,1),
         W=c("quant","quant","qual","quant","qual",
             "qual","quant","qual","qual","quant")) |> 
  dplyr::mutate(Y = ifelse(D=="A",Ya,Yb))

ATE <- mean(data$Ya-data$Yb)
ATE_qual <- mean(data$Ya[data$W=="qual"]) -
  mean(data$Yb[data$W=="qual"])
ATE_quant <- mean(data$Ya[data$W=="quant"]) -
  mean(data$Yb[data$W=="quant"])
DIM <- mean(data$Y[data$D=="A"]) - mean(data$Y[data$D=="B"])
DIM_quant <- mean(data$Y[data$D=="A" & data$W=="quant"]) - 
  mean(data$Y[data$D=="B" & data$W=="quant"])
DIM_qual <- mean(data$Y[data$D=="A" & data$W=="qual"]) - 
  mean(data$Y[data$D=="B" & data$W=="qual"])
weights_quant <- sum(data$W=="quant")/nrow(data)
weights_qual <- sum(data$W=="qual")/nrow(data)
DIM_w <- DIM_quant*weights_quant + DIM_qual*weights_qual
```

::: columns
::: {.column width="50%"}
One advantage of PO is that we can treat them directly as random variables!

So, everything we already know related to probability manipulation still applies here.

The basic calculation device (usually implicit) for that matter is the **science table**.

Basically, the full schedule response of the potential outcomes under different treatment conditions
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | 1        | 0        |
| 2    | A     | 0     | 0        | 0        | 0        |
| 3    | A     | 0     | 0        | 1        | -1       |
| 4    | A     | 1     | 1        | 0        | 1        |
| 5    | A     | 1     | 1        | 1        | 0        |
| 6    | B     | 0     | 1        | 0        | 1        |
| 7    | B     | 0     | 1        | 0        | 1        |
| 8    | B     | 0     | 0        | 0        | 0        |
| 9    | B     | 1     | 0        | 1        | -1       |
| 10   | B     | 1     | 1        | 1        | 0        |
:::
:::

## Average Treatment Effect

::: columns
::: {.column width="50%"}
Imagine we want to compare two educational programs, $a$ and $b$.

We are interested in the employment status of their graduates after a year ($Y$)

The causal effect of the program would be the comparison of the potential outcomes $Y_{ai}$ and $Y_{bi}$, for all units $i$

$$\text{ATE} = E(Y_{ai}) - E(Y_{bi})$$ $$(6/10) - (5/10) = \mathbf{\color{blue}{0.1}}$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | 1        | 0        |
| 2    | A     | 0     | 0        | 0        | 0        |
| 3    | A     | 0     | 0        | 1        | -1       |
| 4    | A     | 1     | 1        | 0        | 1        |
| 5    | A     | 1     | 1        | 1        | 0        |
| 6    | B     | 0     | 1        | 0        | 1        |
| 7    | B     | 0     | 1        | 0        | 1        |
| 8    | B     | 0     | 0        | 0        | 0        |
| 9    | B     | 1     | 0        | 1        | -1       |
| 10   | B     | 1     | 1        | 1        | 0        |
:::
:::

------------------------------------------------------------------------

## Difference-in-means

::: columns
::: {.column width="50%"}
But we don't observe all potential outcomes for all units!

Can we use instead the observational comparison as a proxy?

Let's calculate the different in means across groups, based on the *actual* program each subject attended:

$$\text{diff-in-means} = E(Y_i|X=a) - E(Y_i|X=b)$$ $$(3/5)-(2/5) = \mathbf{\color{red}{0.2}}$$

$$\color{red}{\text{diff-in-means} \neq \text{ATE}}$$

$\color{red}{\text{But why???}}$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | .        | .        |
| 2    | A     | 0     | 0        | .        | .        |
| 3    | A     | 0     | 0        | .        | .        |
| 4    | A     | 1     | 1        | .        | .        |
| 5    | A     | 1     | 1        | .        | .        |
| 6    | B     | 0     | .        | 0        | .        |
| 7    | B     | 0     | .        | 0        | .        |
| 8    | B     | 0     | .        | 0        | .        |
| 9    | B     | 1     | .        | 1        | .        |
| 10   | B     | 1     | .        | 1        | .        |
:::
:::

------------------------------------------------------------------------

## Sources of bias

::: columns
::: {.column width="50%"}
$$E(\text{diff-in-means})$$ $$= E(Y_i|X=a) - E(Y_i|X=b)$$ $$= E(Y_a|X=a) - E(Y_b|X=b)$$ $$= ATE +$$ $$(E[Y_b|X=a] - E[Y_b|X=b])+$$ $$(1-P[X])(ATT-ATC)$$ $$= \mathbf{\color{blue}{0.1}} + \color{red}{0.2 + (0.5)(-0.2) = \mathbf{0.2}}$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | .        | .        |
| 2    | A     | 0     | 0        | .        | .        |
| 3    | A     | 0     | 0        | .        | .        |
| 4    | A     | 1     | 1        | .        | .        |
| 5    | A     | 1     | 1        | .        | .        |
| 6    | B     | 0     | .        | 0        | .        |
| 7    | B     | 0     | .        | 0        | .        |
| 8    | B     | 0     | .        | 0        | .        |
| 9    | B     | 1     | .        | 1        | .        |
| 10   | B     | 1     | .        | 1        | .        |
:::
:::

------------------------------------------------------------------------

## Identification assumptions

::: columns
::: {.column width="50%"}
We need the following condition to be true: $$
Y_d \perp\!\!\!\perp D 
$$

Do we meet that condition here? No! $$(Y_{ai},Y_{bi}) \not\!\perp\!\!\!\perp D$$

Because: $$P(Y_a = y | D=a) \neq P(Y_a = y)$$ $$P(Y_b = y | D=b) \neq P(Y_b = y)$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | 1        | 0        |
| 2    | A     | 0     | 0        | 0        | 0        |
| 3    | A     | 0     | 0        | 1        | -1       |
| 4    | A     | 1     | 1        | 0        | 1        |
| 5    | A     | 1     | 1        | 1        | 0        |
| 6    | B     | 0     | 1        | 0        | 1        |
| 7    | B     | 0     | 1        | 0        | 1        |
| 8    | B     | 0     | 0        | 0        | 0        |
| 9    | B     | 1     | 0        | 1        | -1       |
| 10   | B     | 1     | 1        | 1        | 0        |
:::
:::

------------------------------------------------------------------------

## Identification assumptions

::: columns
::: {.column width="50%"}
What about including another covariate $W$?

Does the following condition holds? $$
Y_d \perp\!\!\!\perp D | W
$$

Not quite either! But still "better" than before, right? right?

Let's define: $$\text{CATE}_{w} = E(Y_a-Y_b|W=w)$$

and the estimator $\widehat{\text{CATE}}_{w} =$ $$E(Y_i|X=a,W=w) - E(Y_i|X=b,W=w)$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|------|-------|-------|----------|----------|----------|-------|
| 1    | A     | 1     | 1        | 1        | 0        | Quant |
| 2    | A     | 0     | 0        | 0        | 0        | Quant |
| 3    | A     | 0     | 0        | 1        | -1       | Qual  |
| 4    | A     | 1     | 1        | 0        | 1        | Quant |
| 5    | A     | 1     | 1        | 1        | 0        | Qual  |
| 6    | B     | 0     | 1        | 0        | 1        | Qual  |
| 7    | B     | 0     | 1        | 0        | 1        | Quant |
| 8    | B     | 0     | 0        | 0        | 0        | Qual  |
| 9    | B     | 1     | 0        | 1        | -1       | Qual  |
| 10   | B     | 1     | 1        | 1        | 0        | Quant |
:::
:::

------------------------------------------------------------------------

## Identification assumptions

::: columns
::: {.column width="50%"}
$$\widehat{\text{CATE}}_{Quant} = \widehat{\text{CATE}}_{Qual} = 0.16$$ $$\text{ATE} = \sum_w\text{CATE}_w P(w)$$ $$\widehat{\text{ATE}} = (0.5)(0.16) + (0.5)(0.16) = 0.16$$

However, look a the true $\text{ATE}_W$: $$\text{CATE}_{Quant} = -0.2$$ $$\text{CATE}_{Qual} = 0.4$$ $$\text{ATE} = (0.5)(-0.2) + (0.5)(0.4) = \color{blue}{\mathbf{0.1}}$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|------|-------|-------|----------|----------|----------|-------|
| 1    | A     | 1     | 1        | 1        | 0        | Quant |
| 2    | A     | 0     | 0        | 0        | 0        | Quant |
| 3    | A     | 0     | 0        | 1        | -1       | Qual  |
| 4    | A     | 1     | 1        | 0        | 1        | Quant |
| 5    | A     | 1     | 1        | 1        | 0        | Qual  |
| 6    | B     | 0     | 1        | 0        | 1        | Qual  |
| 7    | B     | 0     | 1        | 0        | 1        | Quant |
| 8    | B     | 0     | 0        | 0        | 0        | Qual  |
| 9    | B     | 1     | 0        | 1        | -1       | Qual  |
| 10   | B     | 1     | 1        | 1        | 0        | Quant |
:::
:::

------------------------------------------------------------------------

## So, how do we know?

::: columns
::: {.column width="50%"}
In general, we rely on **extra-statistical assumptions** about the data generating process to claim causal identification.

::: callout-tip
## "No causes in, no cases out"

Nancy Cartwright
:::

Is there a way to design an study in which we know, **by design**, that the needed assumptions hold?
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|------|-------|-------|----------|----------|----------|-------|
| 1    | A     | 1     | 1        | .        | .        | Quant |
| 2    | A     | 0     | 0        | .        | .        | Quant |
| 3    | A     | 0     | 0        | .        | .        | Qual  |
| 4    | A     | 1     | 1        | .        | .        | Quant |
| 5    | A     | 1     | 1        | .        | .        | Qual  |
| 6    | B     | 0     | .        | 0        | .        | Qual  |
| 7    | B     | 0     | .        | 0        | .        | Quant |
| 8    | B     | 0     | .        | 0        | .        | Qual  |
| 9    | B     | 1     | .        | 1        | .        | Qual  |
| 10   | B     | 1     | .        | 1        | .        | Quant |
:::
:::

# Randomised Experiments {background-color="#17a091"}

## Why randomisation?

<br>

If we want to [**predict under interventions**]{style="color:blue;"}, then the best way to do it is **interveening**!

. . .

Random assignment has been called the **gold standard** for causal inference: it guarantees the necessary assumptions for causal inference hold by design.

. . .

::: callout-note
What is the difference between random *assignment* and random *sampling*???
:::


## Why randomisation?

<br>

When unfeasible, imagining a hypothetical experiment still offers a useful benchmark to assess the validity of causal claims, and even to clarify [what do we mean]{style="color:blue;"} by a particular causal effect.

Experiments come in many different flavors: lab, field, survey, and even quasi-experiments!

Here we will only scratch the surface of social science experiments: the idea is to get you interested and point you to the resources out there

------------------------------------------------------------------------


## Why randomisation? Intuition

Let's go back for a second to the switching equation introduced earlier: 

. . .

$$
Y = DY_1 + (1-D)Y_0
$$

. . .

Basically, the idea (somewhat counter-intuitive) is that:

* Potential outcomes are fixed ($Y_1$ and $Y_0$ just exists out there)

* The treatment's $D$ sole function is to "reveal" one or another!

Then, it is clear that, [when $D$ is randomly assigned]{style="color:green;"}, we get a random sample of both $Y_1$ and $Y_0$, allowing us to estimate the ATE!

:::aside
We won't discuss it here, but there are ways to conceptualize potential outcomes as random instead of fixed. This mostly affects estimation rather than identification
:::


## Why randomisation?

### (more formally)

We already said that we can identify the causal effect of $D$ on $Y$ if the treatment assignment is independent of the potential outcomes. Formally, when $$(Y_{d=1},Y_{d=0}) \perp\!\!\!\perp D$$


## Why randomisation?

### (more formally)

Recall the diff-in-means bias decomposition we reviewed earlier:

By consistency

$$E(Y_i|D=1) - E(Y_i|D=0) = E(Y_1|D=1) - E(Y_{0}|D=0)$$

Plus some algebra (explicit derivation in Morgan and Winship)

$$= E(Y_1 - Y_{0}) + (E[Y_{0}|D=1] - E[Y_{0}|D=0])+(1-P[D])(ATT-ATC)$$

[Pause! What is the meaning of each term?]{.fragment .fade-in .highlight-blue}


## Why randomisation?

### (more formally)

$$= E(Y_1 - Y_{0}) + (E[Y_{0}|D=1] - E[Y_{0}|D=0])+(1-P[D])(ATT-ATC)$$

Given [**ignorability**]{style="color:red;"} of the treatment assignment, we can further simplify it as:

$$E(Y_1 - Y_{0}) = ATE$$

------------------------------------------------------------------------

## Forms of validity

<br>

Traditionally, researchers have argued about the validity of a study's causal conclusion (and, more generally, about the validity of different research designs) based on the potential biases that pose **threats to validity**. 

Check [this](https://journals.lww.com/epidem/Fulltext/2020/05000/A_Graphical_Catalog_of_Threats_to_Validity_.11.aspx) amazing paper by Matthay and Glymour for a review.

## Forms of validity

<br>

We reviewed the bias in the difference-in-means estimator: baseline differences (under the control condition), and differential response to the treatment (under the exposure condition).

. . .

But when we randomize an exposure, we know that who ends up in each treatment arm has nothing to do with their potential outcomes!

. . .

This is why we generally say that experiments are great for [internal validity]{style="color:green;"}: among the people that participated in our study, we can rule out systematic sources of bias!

However, this does not imply that our results are [externally valid]{style="color:red;"}, i.e., that they apply to people outside our study! We need further assumptions to move from one to another.

## Types of experiments

-   [**Laboratory experiments**]{.fragment .highlight-blue}: Usually conducted with a small sample (of undergraduate psychology students), many times involving games in a computer. Helpful for cognitive/behavioral questions.

-   [**Field experiments**]{.fragment .highlight-blue}: In order to obtain more *externally valid* results, experiments conducted in the field (i.e., under real-world conditions) are the way to go. Definitely more expensive though. Audit studies are a particular type of field experiment.

-   [**Survey experiments**]{.fragment .highlight-blue}: One can randomize treatment conditions *in a survey* to evaluate how participants change their responses based on certain stimulus. Vignettes and list experiments are examples of this approach.

-   [**(Bonus) Quasi-experiments**]{.fragment .highlight-blue}: Researchers usually call quasi-experiments to real-world situations that offer as-if random variation in a treatment of interest. For example, earthquakes, change in laws, date of birth, etc.

------------------------------------------------------------------------


## Short activity (3 mins) {background-color="#692044"}

Sometimes it is hard to imagine an experiment that would be relevant for the type of questions we care about.

Some people even say (and I for sure partially agree!) that experiments tend to emphasize "small" versus "big" questions, promoting incremental/testable policies.

However, there are tons of examples of researchers using experiments to address important, **big** and difficult questions. **Do you know of any example?**

Take a moment to check the syllabus of UCLA professor Graeme Blair's Experimental Design class [here](https://graemeblair.com/teaching/UCLA_PS200E_Syllabus.pdf). He put together a list of experiments conducted by UCLA faculty, and by graduate students.

------------------------------------------------------------------------

## How to randomise?

<br>

We use randomisation not just for identification ([**ignorability**]{style="color:red;"}), but also for estimation!

If we assume the potential outcomes are fixed, and the only thing that varies is the treatment assignment scheme, we can derive a [**permutation distribution**]{.fragment .highlight-blue} and use it for inference.

## How to randomise?

<br>

How much dispersion (i.e. uncertainty) is in our distribution will be affected by [**the level**]{style="color:blue;"} at which randomization (or, more precisely, the treatment) happens: is it at the individual level? or at a cluster/group level?

::: callout-note
The more the aggregation, the more uncertainty. So why would we want to randomize at the cluster level?
:::

## How to randomise?

<br>

Conditional randomization (i.e., blocking) increases efficiency, when we have variables that are highly predictive of the outcome of interest

::: callout-note
One extreme of this is randomization in matched pairs: for each pair of individual with similar covariates, we randomly assign one to treatment and one to control
:::

## Blocking

<br>

Similar to the intuition for stratified random *sampling* in the context of surveys, blocking may increase precision in experimental design

Precision gains are similar to increasing the sample size

* Collect background information on covariates relevant *to the outcome*

* Pre-stratify your sample, then randomise within blocks

  + This ensures that, with respect to the blocked factors, both treatment arms are identical
  
  + It is essentially the same as running a separate experiment in each strata
  
  
## Blocking 

<br>

For estimation, obtain block-specific effects, and average according to population shares. With $J$ strata:

$$
\tau_{\text{block}} = \sum_{j=1}^{J} \frac{N_j}{N} \tau_j
$$
with variance: 

$$V(\tau_{\text{block}}) = \sum_{j=1}^{J} \Big(\frac{N_j}{N}\Big)^2 V(\tau_j)$$

:::aside
When would this be equal to regression with block fixed effects?
:::

## How to estimate?

<br>

Once we have random assignment, estimation is super easy!

All that matters is that your estimation procedure follows your (known) assignment rule

:::{.callout-warning}
## In other words: Estimate as you randomise!
:::

. . . 

* A simple difference in means will recover the ATE

* This is equivalent to running a bivariate regression (no controls)

* With controls is a little trickier (see [Freedman 2008](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-1/On-regression-adjustments-in-experiments-withseveral-treatments/10.1214/07-AOAS143.full) and [Lin 2013](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Agnostic-notes-on-regression-adjustments-to-experimental-data--Reexamining/10.1214/12-AOAS583.full)), but more useful (increased precision)!

* Your standard errors depend on your randomisation scheme! 
  + is assignment at the individual level?
  + do probabilities vary by block?
  + is randomisation conducted among clusters?

## Covariate balance

<br>

When you randomise the treatment, pre-treatment variables will be (in expectation) balanced across treatment arms

This is usually a way to "check" if the randomisation step worked out correctly

However, one should be careful to not discard "unlucky" realizations in an arbitrary manner

. . .

Also, note that imbalance [**does not**]{style="color:red;"} imply bias: properly estimated standard errors will account for uncertainty in the treatment effect!

* Check this great series by Senn: [blog](https://errorstatistics.com/2020/04/20/s-senn-randomisation-is-not-about-balance-nor-about-homogeneity-but-about-randomness-guest-post/), [slides](https://www.ideal.rwth-aachen.de/wp-content/uploads/2014/02/Senn_Randomisation.pdf), [article](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.5713)


## Potential issues

<br>

In random-land, everything is perfect!

However, many things can go wrong in practice, threatenind *internal validity*:

* [**Randomisation failure**]{.fragment .highlight-blue}: for some reason, the physical procedure of assignment fails to be random (see Vietnam draft lottery)

* [**Noncompliance**]{.fragment .highlight-blue}: many times, participants fail to adhere to their assigned treatment status (one- or two-sided noncompliance)

* [**Attrition and loss to follow up**]{.fragment .highlight-blue}: even if randomisation is properly conducted and everyone adheres to the treatment, we may differentially lost participants in the post-treatment period!

## Potential issues

![](https://imgs.xkcd.com/comics/research_ethics.png)

:::aside
[xkcd ethics](https://imgs.xkcd.com/comics/research_ethics.png)

[Lacour affair](https://en.wikipedia.org/wiki/When_Contact_Changes_Minds)
:::


------------------------------------------------------------------------

## Additional Resources

### Online learning



-   J-PAL research resources [here](https://www.povertyactionlab.org/research-resources?view=toc)

-   EGAP methods guides [here](https://egap.org/methods-guides/)



------------------------------------------------------------------------

## Short activity {background-color="#692044"}

Think in a research question that you could possibly address using an experimental design:

-   What is your research question?

-   What is your **estimand**? (effect of what? on what? among whom?)

-   What type of experiment would you conduct? (lab? field? survey?)

-   What would be the level of your randomisation? (individual? cluster? why?)

- What ethical considerations you should take into account for your experiment to be acceptable?
